<!DOCTYPE HTML>
<!-- saved from url=(0043)http://www.xperseverance.net/blogs/tag/lda/ -->
<!DOCTYPE html PUBLIC "" ""><!--[if IE 6]> <html id="ie6" class="no-js" lang="zh-CN"> <![endif]--><!--[if IE 7]> <html id="ie7" class="no-js" lang="zh-CN"> <![endif]--><!--[if IE 8]> <html id="ie8" class="no-js" lang="zh-CN"> <![endif]--><!--[if !(IE 6) | !(IE 7) | !(IE 8)  ]><!--><HTML 
class="no-js" lang="zh-CN"><!--<![endif]--><HEAD><META content="IE=11.0000" 
http-equiv="X-UA-Compatible">
 
<META charset="UTF-8"> 
<META name="viewport" content="width=device-width, initial-scale=1.0"> 
<TITLE>LDA | 持之以恒 </TITLE> <LINK href="http://gmpg.org/xfn/11" rel="profile"> 
<LINK href="LDA%20%20持之以恒_files/style.css" rel="stylesheet" type="text/css" 
media="all"> <LINK href="http://www.xperseverance.net/blogs/xmlrpc.php" rel="pingback"> 
<SCRIPT type="text/x-mathjax-config">
MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</SCRIPT>
 <LINK title="持之以恒 » Feed" href="http://www.xperseverance.net/blogs/feed/" rel="alternate" 
type="application/rss+xml"> <LINK title="持之以恒 » 评论 Feed" href="http://www.xperseverance.net/blogs/comments/feed/" 
rel="alternate" type="application/rss+xml">             
<SCRIPT type="text/javascript">//<![CDATA[
            // Google Analytics for WordPress by Yoast v4.3.3 | http://yoast.com/wordpress/google-analytics/
            var _gaq = _gaq || [];
            _gaq.push(['_setAccount', 'UA-30026507-1']);
				            _gaq.push(['_trackPageview']);
            (function () {
                var ga = document.createElement('script');
                ga.type = 'text/javascript';
                ga.async = true;
                ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';

                var s = document.getElementsByTagName('script')[0];
                s.parentNode.insertBefore(ga, s);
            })();
            //]]></SCRIPT>
			 <LINK title="持之以恒 » LDA 标签 Feed" href="http://www.xperseverance.net/blogs/tag/lda/feed/" 
rel="alternate" type="application/rss+xml"> <LINK id="wppa_style-css" href="LDA%20%20持之以恒_files/wppa-style.css" 
rel="stylesheet" type="text/css" media="all"> 
<SCRIPT src="LDA%20%20持之以恒_files/jquery.js" type="text/javascript"></SCRIPT>
 
<SCRIPT src="LDA%20%20持之以恒_files/wppa.min.js" type="text/javascript"></SCRIPT>
 
<SCRIPT src="LDA%20%20持之以恒_files/modernizr-2.0.6.js" type="text/javascript"></SCRIPT>
 
<SCRIPT src="LDA%20%20持之以恒_files/hoverIntent.js" type="text/javascript"></SCRIPT>
 
<SCRIPT src="LDA%20%20持之以恒_files/superfish.js" type="text/javascript"></SCRIPT>
 
<SCRIPT src="LDA%20%20持之以恒_files/MathJax.js" type="text/javascript"></SCRIPT>
 <LINK title="RSD" href="http://www.xperseverance.net/blogs/xmlrpc.php?rsd" rel="EditURI" 
type="application/rsd+xml"> <LINK href="http://www.xperseverance.net/blogs/wp-includes/wlwmanifest.xml" 
rel="wlwmanifest" type="application/wlwmanifest+xml"> 
<META name="GENERATOR" content="MSHTML 11.00.9600.17924">	 <LINK href="LDA%20%20持之以恒_files/SyntaxHighlighter.css" 
rel="stylesheet" type="text/css">		 
<STYLE type="text/css">
.sf-menu a.sf-with-ul { padding-right: 2.25em;} 
#nav-menu2 a{ line-height: 33px;} 
</STYLE>
<!-- end of style section --> 
<SCRIPT>
	jQuery(function(){jQuery('ul.sf-menu').superfish({animation: {opacity:'show',height:'show'}, speed: 300});});
	</SCRIPT>
 <!-- End of Theme options -->	 
<STYLE type="text/css">.recentcomments a{display:inline !important;padding:0 !important;margin:0 !important;}</STYLE>
 
<SCRIPT type="text/javascript">
//<![CDATA[
	var screen_res = ""; 
	function writeCookie(name,value,hours) {
		var the_cookie = name+"="+escape(value)+"; expires=";
		var expires = "";
		hours=hours+0; //convert to number
		if (hours > 0) { //0==expires on browser close
			var cdate = new Date();
			cdate.setTime(cdate.getTime()+(hours*60*60*1000));
			expires = expires+cdate.toGMTString();
		} 
		document.cookie = the_cookie+expires+"; path=/; domain=";
	}
	screen_res = screen.width+" x "+screen.height;
	if (screen_res==" x ") screen_res = window.screen.width+" x "+window.screen.height;
	if (screen_res==" x ") screen_res = screen.availWidth+" x "+screen.availHeight;
	if (screen_res!=" x ") { 
		writeCookie("wassup_screen_res",screen_res,"48"); //keep 2 days
	} else {
		screen_res = "";
	}
//]]>
</SCRIPT>
 <!-- WPPA+ Runtime parameters -->	 
<SCRIPT type="text/javascript">
	/* <![CDATA[ */

	wppaBackgroundColorImage = "#eeeeee";
	wppaPopupLinkType = "photo";
	wppaAnimationType = "fadeover";
	wppaAnimationSpeed = 800;
	wppaImageDirectory = "http://www.xperseverance.net/blogs/wp-content/plugins/wp-photo-album-plus/images/";
	wppaThumbnailAreaDelta = 9;
	wppaTextFrameDelta = 331;
	wppaBoxDelta = 16;
	wppaSlideShowTimeOut = 2500;
	wppaPreambule = 3;
	wppaFilmShowGlue = true;
	wppaSlideShow = "幻灯片";
	wppaStart = "开始";
	wppaStop = "停止";
	wppaSlower = "较慢";
	wppaFaster = "较快";
	wppaPhoto = "相片";
	wppaOf = "于";
	wppaPreviousPhoto = "Previous photo";
	wppaNextPhoto = "Next photo";
	wppaPrevP = "上一页。";
	wppaNextP = "下一页";
	wppaAvgRating = "平均评级";
	wppaMyRating = "我的评分";
	wppaAvgRat = "Avg.";
	wppaMyRat = "Mine";
	wppaDislikeMsg = "You marked this image as inappropriate.";
	wppaMiniTreshold = 300;
	wppaUserName = "137.111.13.46";
	wppaRatingOnce = false;
	wppaPleaseName = "请输入您的姓名";
	wppaPleaseEmail = "请输入一个有效的的电子邮件地址";
	wppaPleaseComment = "Please enter a comment";
	wppaHideWhenEmpty = false;
	wppaBGcolorNumbar = "#cccccc";
	wppaBcolorNumbar = "#cccccc";
	wppaBGcolorNumbarActive = "#333333";
	wppaBcolorNumbarActive = "#333333";
	wppaFontFamilyNumbar = "";
	wppaFontSizeNumbar = "px";
	wppaFontColorNumbar = "#777777";
	wppaFontWeightNumbar = "bold";
	wppaFontFamilyNumbarActive = "";
	wppaFontSizeNumbarActive = "px";
	wppaFontColorNumbarActive = "#777777";
	wppaFontWeightNumbarActive = "bold";
	wppaNumbarMax = "10";
	wppaLocale = "zh_CN";
	wppaAjaxUrl = "http://www.xperseverance.net/blogs/wp-admin/admin-ajax.php";
	wppaNextOnCallback = false;
	wppaRatingUseAjax = false;
	wppaStarOpacity = 0.2;
	wppaTickImg.src = "http://www.xperseverance.net/blogs/wp-content/plugins/wp-photo-album-plus/images/tick.png";
	wppaClockImg.src = "http://www.xperseverance.net/blogs/wp-content/plugins/wp-photo-album-plus/images/clock.png";
	wppaSlideWrap = true;
	wppaLightBox = "";
	wppaEmailRequired = true;
	wppaSlideBorderWidth = 0;
	wppaAllowAjax = false;
	wppaUsePhotoNamesInUrls = false;
	wppaThumbTargetBlank = false;
	wppaRatingMax = 10;
	wppaRatingDisplayType = "graphic";
	wppaRatingPrec = 2;
	wppaStretch = false;
	wppaMinThumbSpace = 4;
	wppaThumbSpaceAuto = true;
	wppaMagnifierCursor = "magnifier-small.png";
	wppaArtMonkyLink = "none";
	wppaAutoOpenComments = true;
	wppaUpdateAddressLine = true;
	/* ]]> */
</SCRIPT>
 <!-- WPPA+ Rendering enabled --> </HEAD> 
<BODY class="archive tag tag-lda tag-13 two-column ">
<DIV id="head-wrapper"><HEADER id="branding" role="banner">
<DIV class="clearfix" id="header-group">
<DIV id="header-logo"><HGROUP>
<H1 id="site-title"><SPAN><A title="持之以恒" href="http://www.xperseverance.net/blogs/" 
rel="home">持之以恒</A></SPAN></H1>
<H2 id="site-description">Every thing that has a beginning has an 
end.</H2></HGROUP></DIV></DIV></HEADER><!-- #branding -->	 </DIV><!-- #head-wrapper --> 
<DIV class="hfeed" id="page">
<DIV id="nav-bottom-menu">
<DIV id="nav-bottom-wrap"><NAV id="nav-menu2" role="navigation">
<H3 class="assistive-text">Main menu</H3>
<DIV class="skip-link"><A title="Skip to primary content" class="assistive-text" 
href="http://www.xperseverance.net/blogs/tag/lda/#content">Skip to primary 
content</A></DIV>
<DIV class="skip-link"><A title="Skip to secondary content" class="assistive-text" 
href="http://www.xperseverance.net/blogs/tag/lda/#secondary">Skip to secondary 
content</A></DIV>
<DIV class="menu">
<UL class="sf-menu">
  <LI><A title="首页" href="http://www.xperseverance.net/blogs/">首页</A></LI>
  <LI class="page_item page-item-616"><A href="http://www.xperseverance.net/blogs/%e7%95%99%e8%a8%80%e6%9d%bf/">留言板</A></LI>
  <LI class="page_item page-item-643"><A href="http://www.xperseverance.net/blogs/%e7%9b%b8%e5%86%8c/">相册</A></LI>
  <LI class="page_item page-item-254"><A href="http://www.xperseverance.net/blogs/returnhomepage/">关于我</A></LI></UL></DIV>
<FORM id="searchform" action="http://www.xperseverance.net/blogs/" 
method="get"><LABEL class="assistive-text" for="s">Search</LABEL>		 <INPUT name="s" class="field" id="s" type="text" placeholder="Search" value="">
		 <INPUT name="submit" class="submit" id="searchsubmit" type="submit" value="Search">
	 </FORM></NAV><!-- #nav-menu2 -->		 </DIV></DIV>
<DIV id="main"><SECTION id="primary">
<DIV id="content" role="main"><HEADER class="page-header">
<H1 class="page-title">Tag Archives: <SPAN>LDA</SPAN></H1></HEADER><ARTICLE 
class="post-1744 post type-post status-publish format-standard hentry category-academics tag-gibbs-sampling tag-lda tag-mcmc" 
id="post-1744"><HEADER class="entry-header">
<DIV class="calendar"><SPAN class="month">三</SPAN><SPAN 
class="day">05</SPAN></DIV><!-- calendar -->			
<H1 class="entry-title"><A title="Permalink to Reading Note : Parameter estimation for text analysis 暨LDA学习小结" 
href="http://www.xperseverance.net/blogs/2013/03/1744/" rel="bookmark">Reading 
Note : Parameter estimation for text analysis 暨LDA学习小结</A></H1>
<DIV class="entry-meta"><SPAN class="sep">发表日期：</SPAN><A title="下午 2:49" href="http://www.xperseverance.net/blogs/2013/03/1744/" 
rel="bookmark"><time class="entry-date" pubdate="" datetime="2013-03-05T14:49:46+00:00">2013 
年 3 月 5 日</time></A><SPAN class="by-author"> <SPAN class="sep">by </SPAN> <SPAN 
class="author vcard"><A title="View all posts by 恒" class="url fn n" href="http://www.xperseverance.net/blogs/author/xlh/" 
rel="author">恒</A></SPAN></SPAN>			</DIV><!-- .entry-meta -->					 </HEADER><!-- .entry-header -->
				 
<DIV class="entry-content">
<P><SPAN style="font-size: 18px;">伟大的Parameter estimation for text 
analysis！当把这篇看的差不多的时候，也就到了LDA基础知识终结的时刻了，意味着LDA基础模型的基本了解完成了。所以对该模型的学习告一段落，下一阶段就是了解LDA无穷无尽的变种，不过那些不是很有用了，因为LDA已经被人水遍了各大“论坛”……</SPAN></P>
<P><SPAN 
style="font-size: 18px;">抛开LDA背后复杂深入的数学背景不说，光就LDA的内容，确实不多，虽然变分法还是不懂，不过现在终于还是理解了“LDA 
is just a simple model”这句话。</SPAN></P>
<DIV><SPAN style="font-size: 18px;">总结一下学习过程：</SPAN></DIV>
<DIV><SPAN style="font-size: 18px;">1.<A href="http://www.xperseverance.net/blogs/2012/03/516/" 
target="_blank">概率的基本概念</A>：CDF、PDF、Bayes’rule、各种简单的分布Bernoulli，binomial，multinomial、包括对prior、likelihood、postprior的理解（PRML1.2）</SPAN></DIV>
<DIV><SPAN style="font-size: 18px;">2.共轭：<A href="http://www.xperseverance.net/blogs/2012/03/21/" 
target="_blank">为何Beta Distribution与Bernoulli共轭</A>？&nbsp;<A href="http://www.xperseverance.net/blogs/2012/03/510/" 
target="_blank">狄利克雷分布 Dirichlet Distribution</A></SPAN></DIV>
<DIV><SPAN style="font-size: 18px;">​3.概率图模型 Probabilistic Graphical Models: 
PRML Chapter 8 基本概念即可</SPAN></DIV>
<DIV><SPAN style="font-size: 18px;">4.采样算法：<A href="http://www.xperseverance.net/blogs/2012/03/753/" 
target="_blank">Basic Sampling</A>，Sampling Methods（PRML Chapter 11），<A href="http://www.xperseverance.net/blogs/2012/04/902/" 
target="_blank">马尔科夫蒙特卡洛 MCMC</A>，Gibbs Sampling</SPAN></DIV>
<DIV><SPAN style="font-size: 18px;">5.<A href="http://www.xperseverance.net/blogs/2012/03/17/" 
target="_blank">原始论文阅读记录：【JMLR】LDA</A></SPAN></DIV>
<DIV><SPAN style="font-size: 18px;">​6.进阶资料：《<A href="http://www.xperseverance.net/blogs/2013/03/1682/" 
target="_blank">Gibbs Sampling for the Uninitiated</A>》、本文</SPAN></DIV>
<DIV>&nbsp;</DIV>
<DIV><SPAN style="font-size: 18px;">——————————————– 伟大的分割线 
！PETA！&nbsp;​——————————————–</SPAN></DIV>
<P><STRONG><SPAN style="font-size: 18px;">一、前面无关部分</SPAN></STRONG></P>
<P><SPAN style="font-size: 18px;"><A href="http://www.xperseverance.net/blogs/2012/11/1396/" 
target="_blank">关于ML、MAP、Bayesian inference</A></SPAN></P>
<P><STRONG><SPAN style="font-size: 18px;">二、模型进一步记忆</SPAN></STRONG></P>
<P><SPAN style="font-size: 18px;"><IMG style="width: 350px; height: 394px;" alt="" 
src="LDA%20%20持之以恒_files/PETA_LDA_Model.png"></SPAN></P>
<P><SPAN style="font-size: 18px;">从本图来看，需要记住：</SPAN></P>
<P><SPAN 
style="font-size: 18px;">1.$\theta_m$是每一个document单独一个$\theta$，所以M个doc共有M个$\theta_m$，整个$\theta$是一个M*K的矩阵（M个doc，每个doc一个K维topic分布向量）。</SPAN></P>
<P><SPAN 
style="font-size: 18px;">2.$\varphi_k$总共只有K个，对于每一个topic，有一个$\varphi_k$，这些参数是独立于文档的，也就是对于整个corpus只sample一次。不像$\theta_m$那样每一个都对应一个文档，每个文档都不同，$\varphi_k$对于所有文档都相同，是一个K*V的矩阵（K个topic，每个topic一个V维从topic产生词的概率分布）。</SPAN></P>
<P><SPAN style="font-size: 18px;">就这些了。</SPAN></P>
<P><STRONG><SPAN style="font-size: 18px;">三、推导</SPAN></STRONG></P>
<P><SPAN 
style="font-size: 18px;">公式（39）：$P(p|\alpha)=Dir(p|\alpha)$意思是从参数为$\alpha$的狄利克雷分布，采样一个多项分布参数p的概率是多少，概率是标准狄利克雷PDF。这里Dirichlet 
delta function为：</SPAN></P>
<P style="text-align: center;"><SPAN 
style="font-size: 20px;">$\Delta(\vec{\alpha})=\frac{\Gamma(\alpha_1)*\Gamma(\alpha_2)*…*\Gamma(\alpha_k)}{\Gamma(\sum_1^K&nbsp;\alpha_k)}$</SPAN></P>
<P><SPAN style="font-size: 18px;">这个function要记住，下面一溜烟全是这个。</SPAN></P>
<P><SPAN 
style="font-size: 18px;">公式（43）是一元语言模型的likelihood，意思是如果提供了语料库W，知道了W里面每个词的个数，那么使用最大似然估计最大化L就可以估计出参数多项分布p。</SPAN></P>
<P><SPAN 
style="font-size: 18px;">公式（44）是考虑了先验的情形，假如已知语料库W和参数$\alpha$，那么他们产生多项分布参数p的概率是$Dir(p|\alpha+n)$，这个推导我记得在PRML2.1中有解释，抛开复杂的数学证明，只要参考标准狄利克雷分布的归一化项，很容易想出式（46）的归一化项就是$\Delta(\alpha+n)$。这时如果要通过W估计参数p，那么就要使用贝叶斯推断，用这个狄利克雷pdf输出一个p的期望即可。</SPAN></P>
<P><SPAN 
style="font-size: 18px;">最关键的推导（63）-（78）：从63-73的目标是要求出整个LDA的联合概率表达式，这样（63）就可以被用在Gibbs 
Sampler的分子上。首先（63）把联合概率拆成相互独立的两部分$p(w|z,\beta)$和$p(z|\alpha)$，然后分别对这两部分布求表达式。式（64）、（65）首先不考虑超参数$\beta$，而是假设已知参数$\Phi$。这个$\Phi$就是那个K*V维矩阵，表示从每一个topic产生词的概率。然后（66）要把$\Phi$积分掉，这样就可以求出第一部分$p(w|z,\beta)$为表达式（68）。从66-68的积分过程一直在套用狄利克雷积分的结果，反正整篇文章套来套去始终就是这么一个狄利克雷积分。$\vec{n}_z$是一个V维的向量，对于topic 
z，代表每一个词在这个topic里面有几个。从69到72的道理其实和64-68一模一样了。$\vec{n}_m$是一个K维向量，对于文档m，代表每一个topic在这个文档里有几个词。</SPAN></P>
<P><SPAN style="font-size: 18px;">最后（78）求出了Gibbs 
Sampler所需要的条件概率表达式。这个表达式还是要贴出来的，为了和代码里面对应：</SPAN></P>
<P><IMG style="width: 700px; height: 271px;" alt="" src="LDA%20%20持之以恒_files/LDA_Gibbs_Code.png"></P>
<P><SPAN 
style="font-size: 18px;">具体选择下一个新topic的方法是：通过计算每一个topic的新的产生概率$p(z_i=k|z_{\urcorner 
i},w)$也就是代码中的p[k]产生一个新topic。比如有三个topic，算出来产生新的p的概率值为{0.3,0.2,0.4}，注意这个条件概率加起来并不一定是一。然后我为了按照这个概率产生一个新topic，我用random函数从uniform 
distribution产生一个0至0.9的随机数r。如果0&lt;=r&lt;0.3，则新topic赋值为1，如果0.3&lt;=r&lt;0.5，则新topic赋值为2，如果0.5&lt;=r&lt;0.9，那么新topic赋值为3。</SPAN></P>
<P><STRONG style="font-size: 13px;"><SPAN 
style="font-size: 18px;">四、代码</SPAN></STRONG></P>
<PRE class="Java" name="code">/*
 * (C) Copyright 2005, Gregor Heinrich (gregor :: arbylon : net) 
 * LdaGibbsSampler is free software; you can redistribute it and/or modify it
 * under the terms of the GNU General Public License as published by the Free
 * Software Foundation; either version 2 of the License, or (at your option) any
 * later version.
 * LdaGibbsSampler is distributed in the hope that it will be useful, but
 * WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 * FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more
 * details.
 * You should have received a copy of the GNU General Public License along with
 * this program; if not, write to the Free Software Foundation, Inc., 59 Temple
 * Place, Suite 330, Boston, MA 02111-1307 USA
 */
import java.text.DecimalFormat;
import java.text.NumberFormat;

public class LdaGibbsSampler {
    /**
     * document data (term lists)
     */
    int[][] documents;
    /**
     * vocabulary size
     */
    int V;
    /**
     * number of topics
     */
    int K;
    /**
     * Dirichlet parameter (document--topic associations)
     */
    double alpha;
    /**
     * Dirichlet parameter (topic--term associations)
     */
    double beta;
    /**
     * topic assignments for each word.
     * N * M 维，第一维是文档，第二维是word
     */
    int z[][];
    /**
     * nw[i][j] number of instances of word i (term?) assigned to topic j.
     */
    int[][] nw;
    /**
     * nd[i][j] number of words in document i assigned to topic j.
     */
    int[][] nd;
    /**
     * nwsum[j] total number of words assigned to topic j.
     */
    int[] nwsum;
    /**
     * nasum[i] total number of words in document i.
     */
    int[] ndsum;
    /**
     * cumulative statistics of theta
     */
    double[][] thetasum;
    /**
     * cumulative statistics of phi
     */
    double[][] phisum;
    /**
     * size of statistics
     */
    int numstats;
    /**
     * sampling lag (?)
     */
    private static int THIN_INTERVAL = 20;

    /**
     * burn-in period
     */
    private static int BURN_IN = 100;

    /**
     * max iterations
     */
    private static int ITERATIONS = 1000;

    /**
     * sample lag (if -1 only one sample taken)
     */
    private static int SAMPLE_LAG;

    private static int dispcol = 0;

    /**
     * Initialise the Gibbs sampler with data.
     * 
     * @param V
     *            vocabulary size
     * @param data
     */
    public LdaGibbsSampler(int[][] documents, int V) {

        this.documents = documents;
        this.V = V;
    }

    /**
     * Initialisation: Must start with an assignment of observations to topics ?
     * Many alternatives are possible, I chose to perform random assignments
     * with equal probabilities
     * 
     * @param K
     *            number of topics
     * @return z assignment of topics to words
     */
    public void initialState(int K) {
        int i;

        int M = documents.length;

        // initialise count variables.
        nw = new int[V][K];
        nd = new int[M][K];
        nwsum = new int[K];
        ndsum = new int[M];

        // The z_i are are initialised to values in [1,K] to determine the
        // initial state of the Markov chain.
        // 为了方便，他没用从狄利克雷参数采样，而是随机初始化了！

        z = new int[M][];
        for (int m = 0; m &lt; M; m++) {
            int N = documents[m].length;
            z[m] = new int[N];
            for (int n = 0; n &lt; N; n++) {
            	//随机初始化！
                int topic = (int) (Math.random() * K);
                z[m][n] = topic;
                // number of instances of word i assigned to topic j
                // documents[m][n] 是第m个doc中的第n个词
                nw[documents[m][n]][topic]++;
                // number of words in document i assigned to topic j.
                nd[m][topic]++;
                // total number of words assigned to topic j.
                nwsum[topic]++;
            }
            // total number of words in document i
            ndsum[m] = N;
        }
    }

    /**
     * Main method: Select initial state ? Repeat a large number of times: 1.
     * Select an element 2. Update conditional on other elements. If
     * appropriate, output summary for each run.
     * 
     * @param K
     *            number of topics
     * @param alpha
     *            symmetric prior parameter on document--topic associations
     * @param beta
     *            symmetric prior parameter on topic--term associations
     */
    private void gibbs(int K, double alpha, double beta) {
        this.K = K;
        this.alpha = alpha;
        this.beta = beta;

        // init sampler statistics
        if (SAMPLE_LAG &gt; 0) {
            thetasum = new double[documents.length][K];
            phisum = new double[K][V];
            numstats = 0;
        }

        // initial state of the Markov chain:
        //启动马尔科夫链需要一个起始状态
        initialState(K);

        //每一轮sample
        for (int i = 0; i &lt; ITERATIONS; i++) {

            // for all z_i
            for (int m = 0; m &lt; z.length; m++) {
                for (int n = 0; n &lt; z[m].length; n++) {

                    // (z_i = z[m][n])
                    // sample from p(z_i|z_-i, w)
                	//核心步骤，通过论文中表达式（78）为文档m中的第n个词采样新的topic
                    int topic = sampleFullConditional(m, n);
                    z[m][n] = topic;
                }
            }

            // get statistics after burn-in
            //如果当前迭代轮数已经超过 burn-in的限制，并且正好达到 sample lag间隔
            //则当前的这个状态是要计入总的输出参数的，否则的话忽略当前状态，继续sample
            if ((i &gt; BURN_IN) &amp;&amp; (SAMPLE_LAG &gt; 0) &amp;&amp; (i % SAMPLE_LAG == 0)) {
                updateParams();
            }
        }
    }

    /**
     * Sample a topic z_i from the full conditional distribution: p(z_i = j |
     * z_-i, w) = (n_-i,j(w_i) + beta)/(n_-i,j(.) + W * beta) * (n_-i,j(d_i) +
     * alpha)/(n_-i,.(d_i) + K * alpha)
     * 
     * @param m
     *            document
     * @param n
     *            word
     */
    private int sampleFullConditional(int m, int n) {

        // remove z_i from the count variables
    	//这里首先要把原先的topic z(m,n)从当前状态中移除
        int topic = z[m][n];
        nw[documents[m][n]][topic]--;
        nd[m][topic]--;
        nwsum[topic]--;
        ndsum[m]--;

        // do multinomial sampling via cumulative method:
        double[] p = new double[K];
        for (int k = 0; k &lt; K; k++) {
        	//nw 是第i个word被赋予第j个topic的个数
        	//在下式中，documents[m][n]是word id，k为第k个topic
        	//nd 为第m个文档中被赋予topic k的词的个数
            p[k] = (nw[documents[m][n]][k] + beta) / (nwsum[k] + V * beta)
                * (nd[m][k] + alpha) / (ndsum[m] + K * alpha);
        }
        // cumulate multinomial parameters
        for (int k = 1; k &lt; p.length; k++) {
            p[k] += p[k - 1];
        }
        // scaled sample because of unnormalised p[]
        double u = Math.random() * p[K - 1];
        for (topic = 0; topic &lt; p.length; topic++) {
            if (u &lt; p[topic])
                break;
        }

        // add newly estimated z_i to count variables
        nw[documents[m][n]][topic]++;
        nd[m][topic]++;
        nwsum[topic]++;
        ndsum[m]++;

        return topic;
    }

    /**
     * Add to the statistics the values of theta and phi for the current state.
     */
    private void updateParams() {
        for (int m = 0; m &lt; documents.length; m++) {
            for (int k = 0; k &lt; K; k++) {
                thetasum[m][k] += (nd[m][k] + alpha) / (ndsum[m] + K * alpha);
            }
        }
        for (int k = 0; k &lt; K; k++) {
            for (int w = 0; w &lt; V; w++) {
                phisum[k][w] += (nw[w][k] + beta) / (nwsum[k] + V * beta);
            }
        }
        numstats++;
    }

    /**
     * Retrieve estimated document--topic associations. If sample lag &gt; 0 then
     * the mean value of all sampled statistics for theta[][] is taken.
     * 
     * @return theta multinomial mixture of document topics (M x K)
     */
    public double[][] getTheta() {
        double[][] theta = new double[documents.length][K];

        if (SAMPLE_LAG &gt; 0) {
            for (int m = 0; m &lt; documents.length; m++) {
                for (int k = 0; k &lt; K; k++) {
                    theta[m][k] = thetasum[m][k] / numstats;
                }
            }

        } else {
            for (int m = 0; m &lt; documents.length; m++) {
                for (int k = 0; k &lt; K; k++) {
                    theta[m][k] = (nd[m][k] + alpha) / (ndsum[m] + K * alpha);
                }
            }
        }

        return theta;
    }

    /**
     * Retrieve estimated topic--word associations. If sample lag &gt; 0 then the
     * mean value of all sampled statistics for phi[][] is taken.
     * 
     * @return phi multinomial mixture of topic words (K x V)
     */
    public double[][] getPhi() {
        double[][] phi = new double[K][V];
        if (SAMPLE_LAG &gt; 0) {
            for (int k = 0; k &lt; K; k++) {
                for (int w = 0; w &lt; V; w++) {
                    phi[k][w] = phisum[k][w] / numstats;
                }
            }
        } else {
            for (int k = 0; k &lt; K; k++) {
                for (int w = 0; w &lt; V; w++) {
                    phi[k][w] = (nw[w][k] + beta) / (nwsum[k] + V * beta);
                }
            }
        }
        return phi;
    }

    /**
     * Configure the gibbs sampler
     * 
     * @param iterations
     *            number of total iterations
     * @param burnIn
     *            number of burn-in iterations
     * @param thinInterval
     *            update statistics interval
     * @param sampleLag
     *            sample interval (-1 for just one sample at the end)
     */
    public void configure(int iterations, int burnIn, int thinInterval,
        int sampleLag) {
        ITERATIONS = iterations;
        BURN_IN = burnIn;
        THIN_INTERVAL = thinInterval;
        SAMPLE_LAG = sampleLag;
    }

    /**
     * Driver with example data.
     * 
     * @param args
     */
    public static void main(String[] args) {
        // words in documents
        int[][] documents = { {1, 4, 3, 2, 3, 1, 4, 3, 2, 3, 1, 4, 3, 2, 3, 6},
            {2, 2, 4, 2, 4, 2, 2, 2, 2, 4, 2, 2},
            {1, 6, 5, 6, 0, 1, 6, 5, 6, 0, 1, 6, 5, 6, 0, 0},
            {5, 6, 6, 2, 3, 3, 6, 5, 6, 2, 2, 6, 5, 6, 6, 6, 0},
            {2, 2, 4, 4, 4, 4, 1, 5, 5, 5, 5, 5, 5, 1, 1, 1, 1, 0},
            {5, 4, 2, 3, 4, 5, 6, 6, 5, 4, 3, 2}};
        // vocabulary
        int V = 7;
        int M = documents.length;
        // # topics
        int K = 2;
        // good values alpha = 2, beta = .5
        double alpha = 2;
        double beta = .5;

        LdaGibbsSampler lda = new LdaGibbsSampler(documents, V);
        
        //设定sample参数，采样运行10000轮，burn-in 2000轮，第三个参数没用，是为了显示
        //第四个参数是sample lag，这个很重要，因为马尔科夫链前后状态conditional dependent，所以要跳过几个采样
        lda.configure(10000, 2000, 100, 10);
        
        //跑一个！走起！
        lda.gibbs(K, alpha, beta);

        //输出模型参数，论文中式 （81）与（82）
        double[][] theta = lda.getTheta();
        double[][] phi = lda.getPhi();
    }
}
</PRE></DIV><!-- .entry-content -->				 
<DIV style="clear: both;"></DIV><FOOTER class="entry-meta" 
style="clear: both;"><SPAN class="cat-links"><SPAN class="entry-utility-prep entry-utility-prep-cat-links">分类：</SPAN> 
<A title="查看 Academics 中的全部文章" href="http://www.xperseverance.net/blogs/category/academics/" 
rel="category tag">Academics</A>			</SPAN>									 <SPAN class="sep"> 
&nbsp;|&nbsp; </SPAN>							 <SPAN class="tag-links"><SPAN class="entry-utility-prep entry-utility-prep-tag-links">标签：</SPAN> 
<A href="http://www.xperseverance.net/blogs/tag/gibbs-sampling/" rel="tag">Gibbs 
Sampling</A>, <A href="http://www.xperseverance.net/blogs/tag/lda/" 
rel="tag">LDA</A>, <A href="http://www.xperseverance.net/blogs/tag/mcmc/" rel="tag">MCMC</A>			</SPAN>
															 <SPAN class="sep"> &nbsp;|&nbsp; </SPAN>						 <SPAN class="comments-link"><A 
title="《Reading Note : Parameter estimation for text analysis 暨LDA学习小结》上的评论" 
href="http://www.xperseverance.net/blogs/2013/03/1744/#comments"><B>26</B> 
Replies</A></SPAN>                         <!--Added By Xuliheng-->             
<SPAN class="sep"> &nbsp;|&nbsp; </SPAN>            10,766 views            			
					 </FOOTER><!-- #entry-meta -->	 </ARTICLE><!-- #post-1744 -->									
	 <ARTICLE class="post-657 post type-post status-publish format-standard hentry category-academics tag-lda" 
id="post-657"><HEADER class="entry-header">
<DIV class="calendar"><SPAN class="month">三</SPAN><SPAN 
class="day">27</SPAN></DIV><!-- calendar -->			
<H1 class="entry-title"><A title="Permalink to 【转】LDA必读的资料" href="http://www.xperseverance.net/blogs/2012/03/657/" 
rel="bookmark">【转】LDA必读的资料</A></H1>
<DIV class="entry-meta"><SPAN class="sep">发表日期：</SPAN><A title="下午 5:26" href="http://www.xperseverance.net/blogs/2012/03/657/" 
rel="bookmark"><time class="entry-date" pubdate="" datetime="2012-03-27T17:26:00+00:00">2012 
年 3 月 27 日</time></A><SPAN class="by-author"> <SPAN class="sep">by </SPAN> <SPAN 
class="author vcard"><A title="View all posts by 恒" class="url fn n" href="http://www.xperseverance.net/blogs/author/xlh/" 
rel="author">恒</A></SPAN></SPAN>			</DIV><!-- .entry-meta -->					 </HEADER><!-- .entry-header -->
				 
<DIV class="entry-content">
<DIV><SPAN style="font-size: 18px;"><SPAN 
style="color: rgb(0, 0, 255);">2012@4@15添加，一个大牛写的介绍，貌似需翻墙</SPAN></SPAN></DIV>
<DIV><SPAN style="font-size: 18px;"><A onclick="javascript:_gaq.push(['_trackEvent','outbound-article','http://tedunderwood.wordpress.com']);" 
href="http://tedunderwood.wordpress.com/2012/04/07/topic-modeling-made-just-simple-enough/">http://tedunderwood.wordpress.com/2012/04/07/topic-modeling-made-just-simple-enough/</A></SPAN></DIV>
<P><SPAN style="color: rgb(0, 0, 255);"><SPAN 
style="font-size: 18px;">2012@4@18</SPAN></SPAN><SPAN 
style="font-size: 18px;"><SPAN style="color: rgb(0, 0, 255);">添加David 
M.Blei主页</SPAN>：</SPAN><A style="font-size: 18px;" onclick="javascript:_gaq.push(['_trackEvent','outbound-article','http://www.cs.princeton.edu']);" 
href="http://www.cs.princeton.edu/~blei/publications.html">http://www.cs.princeton.edu/~blei/publications.html</A><SPAN 
style="font-size: 18px;">，上面有布雷最新的文章：</SPAN><A style="font-size: 18px;" onclick="javascript:_gaq.push(['_trackEvent','download','http://www.cs.princeton.edu/~blei/papers/Blei2011.pdf']);" 
href="http://www.cs.princeton.edu/~blei/papers/Blei2011.pdf" 
target="_blank">Introduction to probabilistic topic models</A></P>
<P><SPAN 
style="color: rgb(255, 0, 0); font-size: 20px;">以下内容来自网络，但是作者已经不可考啦，抱歉没法找到原始引用</SPAN></P>
<DIV><SPAN style="font-size: 18px;">关于LDA并行化：</SPAN></DIV>
<DIV>
<DIV><SPAN style="font-size: 18px;">那么若利用MapReduce实现，怎样的近似方法好呢？</SPAN></DIV>
<DIV><SPAN style="font-size: 18px;">斯坦福的ScalaNLP项目值得一看：</SPAN></DIV>
<DIV><SPAN 
style="font-size: 18px;">http://nlp.stanford.edu/javanlp/scala/scaladoc/scalanlp/cluster/DistributedGibbsLDA$object.html</SPAN></DIV>
<DIV>&nbsp;</DIV>
<DIV><SPAN style="font-size: 18px;">另外还有NIPS2007的论文：</SPAN></DIV>
<DIV><SPAN style="font-size: 18px;">Distributed Inference for Latent 
DirichletAllocation 
http://books.nips.cc/papers/files/nips20/NIPS2007_0672</SPAN></DIV>
<DIV>&nbsp;</DIV>
<DIV><SPAN style="font-size: 18px;">ICML2008的论文：</SPAN></DIV>
<DIV><SPAN style="font-size: 18px;">Fully Distributed EM for Very Large 
Datasetshttp://www.cs.berkeley.edu/~jawolfe/pubs/08-icml-em</SPAN></DIV>
<DIV>&nbsp;</DIV></DIV>
<P align="center"><A class="more-link" href="http://www.xperseverance.net/blogs/2012/03/657/#more-657">Continue 
reading <SPAN class="meta-nav">→</SPAN></A></P></DIV><!-- .entry-content -->
				 <DIV style="clear: both;"></DIV><FOOTER class="entry-meta" style="clear: both;"><SPAN 
class="cat-links"><SPAN 
class="entry-utility-prep entry-utility-prep-cat-links">分类：</SPAN> <A title="查看 Academics 中的全部文章" 
href="http://www.xperseverance.net/blogs/category/academics/" 
rel="category tag">Academics</A>			</SPAN>									 <SPAN class="sep"> 
&nbsp;|&nbsp; </SPAN>							 <SPAN class="tag-links"><SPAN class="entry-utility-prep entry-utility-prep-tag-links">标签：</SPAN> 
<A href="http://www.xperseverance.net/blogs/tag/lda/" 
rel="tag">LDA</A>			</SPAN>															 <SPAN class="sep"> &nbsp;|&nbsp; 
</SPAN>						 <SPAN class="comments-link"><A title="《【转】LDA必读的资料》上的评论" href="http://www.xperseverance.net/blogs/2012/03/657/#comments"><B>2</B> 
Replies</A></SPAN>                         <!--Added By Xuliheng-->             
<SPAN class="sep"> &nbsp;|&nbsp; </SPAN>            8,604 views            			
					 </FOOTER><!-- #entry-meta -->	 </ARTICLE><!-- #post-657 -->									
	 <ARTICLE class="post-655 post type-post status-publish format-standard hentry category-academics tag-lda" 
id="post-655"><HEADER class="entry-header">
<DIV class="calendar"><SPAN class="month">三</SPAN><SPAN 
class="day">27</SPAN></DIV><!-- calendar -->			
<H1 class="entry-title"><A title="Permalink to GibbsLDA++ 使用记录" href="http://www.xperseverance.net/blogs/2012/03/655/" 
rel="bookmark">GibbsLDA++ 使用记录</A></H1>
<DIV class="entry-meta"><SPAN class="sep">发表日期：</SPAN><A title="下午 5:24" href="http://www.xperseverance.net/blogs/2012/03/655/" 
rel="bookmark"><time class="entry-date" pubdate="" datetime="2012-03-27T17:24:36+00:00">2012 
年 3 月 27 日</time></A><SPAN class="by-author"> <SPAN class="sep">by </SPAN> <SPAN 
class="author vcard"><A title="View all posts by 恒" class="url fn n" href="http://www.xperseverance.net/blogs/author/xlh/" 
rel="author">恒</A></SPAN></SPAN>			</DIV><!-- .entry-meta -->					 </HEADER><!-- .entry-header -->
				 
<DIV class="entry-content">
<DIV><SPAN style="font-size: 18px;">有一个Topic Model的工具Mark一下：<A onclick="javascript:_gaq.push(['_trackEvent','outbound-article','http://mallet.cs.umass.edu']);" 
href="http://mallet.cs.umass.edu/" 
target="_blank">http://mallet.cs.umass.edu/</A></SPAN></DIV>
<DIV>&nbsp;</DIV>
<DIV><SPAN 
style="font-size: 18px;">鄙人一直认为，要了解一样东西首先就要先搜搜，哈哈，搜索引擎真是伟大的发明啊。</SPAN></DIV>
<P><SPAN style="font-size: 18px;">首先是下载页：</SPAN></P>
<P><SPAN style="font-size: 18px;"><A onclick="javascript:_gaq.push(['_trackEvent','outbound-article','http://gibbslda.sourceforge.net']);" 
href="http://gibbslda.sourceforge.net/" 
target="_blank">http://gibbslda.sourceforge.net/</A>&nbsp; 
这个不必多说，好像是一个越南人写的一个集合</SPAN></P>
<P><SPAN style="font-size: 18px;">搜到如下几个好链接：</SPAN></P>
<P><SPAN style="font-size: 18px;">1. 
一个香港城市大学牛人写的很好很基础的GibbsLDA用法，呵呵，人家学文科的也把程序玩那么好，牛！</SPAN></P>
<DIV><A onclick="javascript:_gaq.push(['_trackEvent','outbound-article','http://weblab.com.cityu.edu.hk']);" 
href="http://weblab.com.cityu.edu.hk/blog/luheng/2011/06/24/%E7%94%A8gibbslda%E5%81%9Atopic-modeling/#comment-87" 
target="_blank"><SPAN 
style="font-size: 18px;">http://weblab.com.cityu.edu.hk/blog/luheng/2011/06/24/%E7%94%A8gibbslda%E5%81%9Atopic-modeling/#comment-87</SPAN></A></DIV>
<DIV>&nbsp;</DIV>
<DIV><SPAN style="font-size: 18px;">2. 找到了北大大牛，赵鑫，它的微博<A onclick="javascript:_gaq.push(['_trackEvent','outbound-article','http://weibo.com']);" 
href="http://weibo.com/batmanfly" 
target="_blank">http://weibo.com/batmanfly</A>，他关于LDA的tutorial：<A onclick="javascript:_gaq.push(['_trackEvent','download','http://net.pku.edu.cn/~zhaoxin/Topic-model-xin-zhao-wayne.pdf']);" 
href="http://net.pku.edu.cn/~zhaoxin/Topic-model-xin-zhao-wayne.pdf" target="_blank">http://net.pku.edu.cn/~zhaoxin/Topic-model-xin-zhao-wayne.pdf</A></SPAN></DIV>
<DIV>&nbsp;</DIV>
<DIV><SPAN style="font-size: 18px;">3. 找到了一份 LDA 必读资料，转到自己博客上等有空翻翻哈。在<A href="http://www.xperseverance.net/blogs/2012/03/657/" 
target="_blank">这里</A>。</SPAN></DIV>
<DIV>&nbsp;</DIV>
<DIV><SPAN style="font-size: 18px;">——————————————— 伟大的分割线 
————————————————-</SPAN></DIV>
<DIV>&nbsp;</DIV>
<DIV><SPAN style="font-size: 18px;">好了，现在正式开始 GibbsLDA使用记录，现在2012@3@27 
&nbsp;17:30，其实我还没开始用~电脑还在Windows状态呢，等吃完饭回来切成Linux跑跑，然后上来写使用记录。</SPAN></DIV>
<DIV><SPAN style="font-size: 18px;">把GibbsLDA++0.2 
上传服务器编译后，果真不出所料，出错。根据偶的经验，凡是下载来的源代码，直接编译，没有一个能一下就通过的！</SPAN></DIV>
<DIV><SPAN style="font-size: 18px;">还好这次只是小case问题：</SPAN></DIV>
<DIV><SPAN style="font-size: 18px;">1. utils.cpp 找不到 'atof'函数，补上include： 
#include &lt;cstdlib&gt;</SPAN></DIV>
<DIV><SPAN style="font-size: 18px;">2. lda.cpp 找不到 'printf' 函数，补上 #include 
&lt;cstdio&gt;</SPAN></DIV>
<P><SPAN 
style="font-size: 18px;">pass！就是这么的简单，还是蛮超乎想象的，看来最近新写的工具还是很不错的，没有一大堆外部tool，也没一大堆东东要装，哈。</SPAN></P>
<P><SPAN 
style="font-size: 18px;">好了GibbsLDA编译完成，过几天来写试用报告，和使用感受，最近先看些LDA具体应用。打算实现一下清华<A 
onclick="javascript:_gaq.push(['_trackEvent','outbound-article','http://sixiance.net']);" 
href="http://sixiance.net/" target="_blank">司宪策大牛</A>的《Tag-LDA for Scalable 
Real-time Tag Recommendation》Journal of Computational Information Systems 6(2), 
2009. <SPAN style="color: rgb(0, 0, 255);">今天找到了大牛的主页~3@28</SPAN></SPAN></P>
<P><SPAN style="font-size: 18px;">下文</SPAN></P></DIV><!-- .entry-content -->
				 <DIV style="clear: both;"></DIV><FOOTER class="entry-meta" style="clear: both;"><SPAN 
class="cat-links"><SPAN 
class="entry-utility-prep entry-utility-prep-cat-links">分类：</SPAN> <A title="查看 Academics 中的全部文章" 
href="http://www.xperseverance.net/blogs/category/academics/" 
rel="category tag">Academics</A>			</SPAN>									 <SPAN class="sep"> 
&nbsp;|&nbsp; </SPAN>							 <SPAN class="tag-links"><SPAN class="entry-utility-prep entry-utility-prep-tag-links">标签：</SPAN> 
<A href="http://www.xperseverance.net/blogs/tag/lda/" 
rel="tag">LDA</A>			</SPAN>															 <SPAN class="sep"> &nbsp;|&nbsp; 
</SPAN>						 <SPAN class="comments-link"><A title="《GibbsLDA++ 使用记录》上的评论" href="http://www.xperseverance.net/blogs/2012/03/655/#comments"><B>1</B> 
Reply</A></SPAN>                         <!--Added By Xuliheng-->             
<SPAN class="sep"> &nbsp;|&nbsp; </SPAN>            5,819 views            			
					 </FOOTER><!-- #entry-meta -->	 </ARTICLE><!-- #post-655 -->									
	 <ARTICLE class="post-17 post type-post status-publish format-standard hentry category-academics tag-lda tag-paper-comments" 
id="post-17"><HEADER class="entry-header">
<DIV class="calendar"><SPAN class="month">三</SPAN><SPAN 
class="day">22</SPAN></DIV><!-- calendar -->			
<H1 class="entry-title"><A title="Permalink to 【JMLR’03】Latent Dirichlet Allocation （LDA）- David M.Blei" 
href="http://www.xperseverance.net/blogs/2012/03/17/" 
rel="bookmark">【JMLR’03】Latent Dirichlet Allocation （LDA）- David M.Blei</A></H1>
<DIV class="entry-meta"><SPAN class="sep">发表日期：</SPAN><A title="下午 4:30" href="http://www.xperseverance.net/blogs/2012/03/17/" 
rel="bookmark"><time class="entry-date" pubdate="" datetime="2012-03-22T16:30:00+00:00">2012 
年 3 月 22 日</time></A><SPAN class="by-author"> <SPAN class="sep">by </SPAN> <SPAN 
class="author vcard"><A title="View all posts by 管理员" class="url fn n" href="http://www.xperseverance.net/blogs/author/xuliheng/" 
rel="author">管理员</A></SPAN></SPAN>			</DIV><!-- .entry-meta -->					 </HEADER><!-- .entry-header -->
				 
<DIV class="entry-content">
<P><SPAN style="font-size: 18px;">【注：本文为原创】</SPAN></P>
<P><SPAN style="font-size: 18px;">若公式显示有问题请复制链接到新TAB重新打开</SPAN></P>
<P><SPAN style="font-size: 18px;">听说国外大牛都认为LDA只是很简单的模型，吾辈一听这话，只能加油了~</SPAN></P>
<P><SPAN style="font-size: 18px;">另外这个大牛写的LDA导读很不错：<A onclick="javascript:_gaq.push(['_trackEvent','outbound-article','http://bbs.byr.cn']);" 
href="http://bbs.byr.cn/#!article/PR_AI/2530?p=1" 
target="_blank">http://bbs.byr.cn/#!article/PR_AI/2530?p=1</A></SPAN></P>
<H2><SPAN style="font-size: 18px;">一、预备知识：</SPAN></H2>
<P><SPAN style="font-size: 18px;">&nbsp; &nbsp; &nbsp; &nbsp;1. 
概率密度和二项分布、多项分布，在<A href="http://www.xperseverance.net/blogs/2012/03/516/" 
target="_blank">这里</A></SPAN></P>
<P><SPAN style="font-size: 18px;">&nbsp; &nbsp; &nbsp; &nbsp;2. 狄利克雷分布，在<A href="http://www.xperseverance.net/blogs/2012/03/510/" 
target="_blank">这里</A>，主要内容摘自《Pattern Recognition and Machine 
Learning》第二章</SPAN></P>
<P><SPAN style="font-size: 18px;">&nbsp; &nbsp; &nbsp; &nbsp;3. 
概率图模型，在PRML第九章有很好的介绍</SPAN></P>
<H2><SPAN style="font-size: 18px;">二、变量表示：</SPAN></H2>
<P><SPAN style="font-size: 18px;">&nbsp; &nbsp; &nbsp; 1. 
word：word是最基本的离散概念，在自然语言处理的应用中，就是词。我觉得比较泛化的定义应该是观察数据的最基本的离散单元。word的表示可以是一个V维向量v，V是所有word的个数。这个向量v只有一个值等于1，其他等于0。呵呵，这种数学表示好浪费，我以前做过的项目里一般中文词在200-300w左右，每一个都表示成300w维向量的话就不用活了。哈哈，所以真正应用中word只要一个编号表示就成了。</SPAN></P>
<P><SPAN style="font-size: 18px;">&nbsp; &nbsp; &nbsp;2. 
document：一个document就是多个word的合体。假设一篇文档有N个词，这些word是不计顺序的，也就是exchangeable的，LDA论文 
3.1有说这个概念。论文中document的个数是M。</SPAN></P>
<P><SPAN style="font-size: 18px;">&nbsp; &nbsp; &nbsp;3. 
topic：就是主题啦，比如“钱”的主题可能是“经济”，也可能是“犯罪”~ 
LDA中主题的表示是隐含的，即只预先确定主题的个数，而不知道具体的主题是什么。论文中表示主题个数的字母是k，表示主题的随机变量是z。</SPAN></P>
<P><SPAN 
style="font-size: 18px;">好了，总结一下所有的变量的意思，V是所有单词的个数（固定值），N是单篇文档词的个数（随机变量），M是总的文档的个数（固定值），k是主题的个数（需要预先根据先验知识指定，固定值）。</SPAN></P>
<P align="center"><A class="more-link" href="http://www.xperseverance.net/blogs/2012/03/17/#more-17">Continue 
reading <SPAN class="meta-nav">→</SPAN></A></P></DIV><!-- .entry-content -->
				 <DIV style="clear: both;"></DIV><FOOTER class="entry-meta" style="clear: both;"><SPAN 
class="cat-links"><SPAN 
class="entry-utility-prep entry-utility-prep-cat-links">分类：</SPAN> <A title="查看 Academics 中的全部文章" 
href="http://www.xperseverance.net/blogs/category/academics/" 
rel="category tag">Academics</A>			</SPAN>									 <SPAN class="sep"> 
&nbsp;|&nbsp; </SPAN>							 <SPAN class="tag-links"><SPAN class="entry-utility-prep entry-utility-prep-tag-links">标签：</SPAN> 
<A href="http://www.xperseverance.net/blogs/tag/lda/" rel="tag">LDA</A>, <A 
href="http://www.xperseverance.net/blogs/tag/paper-comments/" rel="tag">Paper 
Comments</A>			</SPAN>															 <SPAN class="sep"> &nbsp;|&nbsp; </SPAN>
						 <SPAN class="comments-link"><A title="《【JMLR’03】Latent Dirichlet Allocation （LDA）- David M.Blei》上的评论" 
href="http://www.xperseverance.net/blogs/2012/03/17/#comments"><B>17</B> 
Replies</A></SPAN>                         <!--Added By Xuliheng-->             
<SPAN class="sep"> &nbsp;|&nbsp; </SPAN>            22,779 views            			
					 </FOOTER><!-- #entry-meta -->	 </ARTICLE><!-- #post-17 -->									
	 <ARTICLE class="post-16 post type-post status-publish format-standard hentry category-academics tag-lda" 
id="post-16"><HEADER class="entry-header">
<DIV class="calendar"><SPAN class="month">三</SPAN><SPAN 
class="day">07</SPAN></DIV><!-- calendar -->			
<H1 class="entry-title"><A title="Permalink to 【转】LDA论文导读" href="http://www.xperseverance.net/blogs/2012/03/16/" 
rel="bookmark">【转】LDA论文导读</A></H1>
<DIV class="entry-meta"><SPAN class="sep">发表日期：</SPAN><A title="上午 9:46" href="http://www.xperseverance.net/blogs/2012/03/16/" 
rel="bookmark"><time class="entry-date" pubdate="" datetime="2012-03-07T09:46:00+00:00">2012 
年 3 月 7 日</time></A><SPAN class="by-author"> <SPAN class="sep">by </SPAN> <SPAN 
class="author vcard"><A title="View all posts by 管理员" class="url fn n" href="http://www.xperseverance.net/blogs/author/xuliheng/" 
rel="author">管理员</A></SPAN></SPAN>			</DIV><!-- .entry-meta -->					 </HEADER><!-- .entry-header -->
				 
<DIV class="entry-content">
<P style="margin: 4px 0px; padding: 2px 0px;">转载自：&nbsp;<A onclick="javascript:_gaq.push(['_trackEvent','outbound-article','http://bbs.byr.cn']);" 
href="http://bbs.byr.cn/#!article/PR_AI/2530?p=1" _xhe_href="http://bbs.byr.cn/#!article/PR_AI/2530?p=1">http://bbs.byr.cn/#!article/PR_AI/2530?p=1</A></P>
<P style="margin: 4px 0px; padding: 2px 0px;">&nbsp;</P>
<P style="margin: 4px 0px; padding: 2px 0px;">原文的主要内容&nbsp;<BR>	&nbsp;&nbsp;<BR>
	&nbsp; &nbsp; &nbsp;有两种方法设计分类器：&nbsp;<BR>	&nbsp; &nbsp; &nbsp;1. discriminative 
model，就是由样本直接设计判别函数，例如SVM；&nbsp;<BR>	&nbsp; &nbsp; &nbsp;2. generative 
model，就是先从样本恢复概率模型——例如我们熟悉的参数方法：混合高斯模型GMM;非参数方法Parzen窗。然后再充分挖掘模型，用以分类。例如Bayes最大后验概率准则；或者将模型中的参数当作提取的特征（参数一般都比较少，所以这么做实际上是在降维），在这些新特征上设计分类器（例如又用SVM）。 
&nbsp; &nbsp;&nbsp;<BR>	&nbsp; &nbsp; 
&nbsp;恢复的模型可生成新的样本，所以得名generative。&nbsp;<BR>	&nbsp;&nbsp;<BR>	&nbsp; &nbsp; 
&nbsp;原文就是讲了一种建立generative model的方法，用于文本处理。&nbsp;<BR>	&nbsp; &nbsp; 
&nbsp;对文本（document）中各单词（word）的出现频率（简称词频）建立概率模型通常是文本处理的第一步。&nbsp;<BR>
	&nbsp;&nbsp;<BR>	&nbsp; &nbsp; &nbsp;开始讨论前，先做如下约定：&nbsp;<BR>	&nbsp; &nbsp; 
&nbsp;- 仅考虑文本的词频，而不考虑单词在文本中出现的先后顺序及其约束关系&nbsp;<BR>	&nbsp; &nbsp; &nbsp;- 
文本中的单词来自大小为|V|的词汇表。例如： V = {FILM, MUSIC, TAX, MILLION, STUDENT, TEACHER, 
SCHOOL}. |V| = 7&nbsp;<BR>	&nbsp; &nbsp; &nbsp;- 每篇文本有N个单词&nbsp;<BR>	&nbsp; 
&nbsp; &nbsp;- 文本来自k个主题（topic）。例如: T = {Arts, Budgets, Education}. k = 
3&nbsp;<BR>	&nbsp;&nbsp;</P>
<P style="text-align: center;"><A class="more-link" href="http://www.xperseverance.net/blogs/2012/03/16/#more-16">Continue 
reading <SPAN class="meta-nav">→</SPAN></A></P></DIV><!-- .entry-content -->
				 <DIV style="clear: both;"></DIV><FOOTER class="entry-meta" style="clear: both;"><SPAN 
class="cat-links"><SPAN 
class="entry-utility-prep entry-utility-prep-cat-links">分类：</SPAN> <A title="查看 Academics 中的全部文章" 
href="http://www.xperseverance.net/blogs/category/academics/" 
rel="category tag">Academics</A>			</SPAN>									 <SPAN class="sep"> 
&nbsp;|&nbsp; </SPAN>							 <SPAN class="tag-links"><SPAN class="entry-utility-prep entry-utility-prep-tag-links">标签：</SPAN> 
<A href="http://www.xperseverance.net/blogs/tag/lda/" 
rel="tag">LDA</A>			</SPAN>															 <SPAN class="sep"> &nbsp;|&nbsp; 
</SPAN>						 <SPAN class="comments-link"><A title="《【转】LDA论文导读》上的评论" href="http://www.xperseverance.net/blogs/2012/03/16/#comments"><B>1</B> 
Reply</A></SPAN>                         <!--Added By Xuliheng-->             
<SPAN class="sep"> &nbsp;|&nbsp; </SPAN>            4,248 views            			
					 </FOOTER><!-- #entry-meta -->	 </ARTICLE><!-- #post-16 -->											
			 </DIV><!-- #content -->		 </SECTION><!-- #primary -->		 
<DIV class="widget-area" id="secondary" role="complementary"><ASIDE class="widget widget_tag_cloud" 
id="tag_cloud-2">
<H3 class="widget-title">标签</H3>
<DIV class="tagcloud"><A title="1 个话题" class="tag-link-9" style="font-size: 8pt;" 
href="http://www.xperseverance.net/blogs/tag/android/">Android</A> <A title="5 个话题" 
class="tag-link-49" style="font-size: 16.615pt;" href="http://www.xperseverance.net/blogs/tag/conferences/">Conferences</A> 
<A title="1 个话题" class="tag-link-38" style="font-size: 8pt;" href="http://www.xperseverance.net/blogs/tag/daily-readings/">Daily 
Readings</A> <A title="2 个话题" class="tag-link-10" style="font-size: 11.23pt;" 
href="http://www.xperseverance.net/blogs/tag/database/">Database</A> <A title="4 个话题" 
class="tag-link-51" style="font-size: 15.179pt;" href="http://www.xperseverance.net/blogs/tag/deep-learning/">Deep 
Learning</A> <A title="1 个话题" class="tag-link-11" style="font-size: 8pt;" href="http://www.xperseverance.net/blogs/tag/english-learning/">English 
Learning</A> <A title="2 个话题" class="tag-link-47" style="font-size: 11.23pt;" 
href="http://www.xperseverance.net/blogs/tag/gibbs-sampling/">Gibbs Sampling</A> 
<A title="1 个话题" class="tag-link-52" style="font-size: 8pt;" href="http://www.xperseverance.net/blogs/tag/latex/">Latex</A> 
<A title="5 个话题" class="tag-link-13" style="font-size: 16.615pt;" href="http://www.xperseverance.net/blogs/tag/lda/">LDA</A> 
<A title="9 个话题" class="tag-link-14" style="font-size: 20.564pt;" href="http://www.xperseverance.net/blogs/tag/linux/">Linux</A> 
<A title="1 个话题" class="tag-link-30" style="font-size: 8pt;" href="http://www.xperseverance.net/blogs/tag/mathematics/">Mathematics</A> 
<A title="2 个话题" class="tag-link-41" style="font-size: 11.23pt;" href="http://www.xperseverance.net/blogs/tag/matrix/">Matrix</A> 
<A title="3 个话题" class="tag-link-33" style="font-size: 13.384pt;" href="http://www.xperseverance.net/blogs/tag/mcmc/">MCMC</A> 
<A title="1 个话题" class="tag-link-31" style="font-size: 8pt;" href="http://www.xperseverance.net/blogs/tag/multithread/">Multithread</A> 
<A title="1 个话题" class="tag-link-15" style="font-size: 8pt;" href="http://www.xperseverance.net/blogs/tag/ontology/">Ontology</A> 
<A title="4 个话题" class="tag-link-50" style="font-size: 15.179pt;" href="http://www.xperseverance.net/blogs/tag/opinion-mining/">Opinion 
Mining</A> <A title="6 个话题" class="tag-link-16" style="font-size: 17.871pt;" 
href="http://www.xperseverance.net/blogs/tag/paper-comments/">Paper Comments</A> 
<A title="11 个话题" class="tag-link-24" style="font-size: 22pt;" href="http://www.xperseverance.net/blogs/tag/prml/">PR&amp;ML</A> 
<A title="5 个话题" class="tag-link-29" style="font-size: 16.615pt;" href="http://www.xperseverance.net/blogs/tag/probability/">Probability</A> 
<A title="1 个话题" class="tag-link-42" style="font-size: 8pt;" href="http://www.xperseverance.net/blogs/tag/programming/">Programming</A> 
<A title="1 个话题" class="tag-link-46" style="font-size: 8pt;" href="http://www.xperseverance.net/blogs/tag/spectral-graph/">Spectral 
Graph</A> <A title="1 个话题" class="tag-link-20" style="font-size: 8pt;" href="http://www.xperseverance.net/blogs/tag/wdk/">WDK</A> 
<A title="1 个话题" class="tag-link-21" style="font-size: 8pt;" href="http://www.xperseverance.net/blogs/tag/web-programming/">Web 
Programming</A> <A title="1 个话题" class="tag-link-22" style="font-size: 8pt;" 
href="http://www.xperseverance.net/blogs/tag/windows-programming/">Windows 
Programming</A> <A title="1 个话题" class="tag-link-37" style="font-size: 8pt;" 
href="http://www.xperseverance.net/blogs/tag/%e5%86%99%e4%bd%9c%e6%8a%80%e5%b7%a7/">写作技巧</A> 
<A title="1 个话题" class="tag-link-44" style="font-size: 8pt;" href="http://www.xperseverance.net/blogs/tag/%e5%92%96%e5%95%a1/">咖啡</A> 
<A title="2 个话题" class="tag-link-54" style="font-size: 11.23pt;" href="http://www.xperseverance.net/blogs/tag/%e5%af%92%e9%97%a8%e9%9a%be%e5%86%8d%e5%87%ba%e8%b4%b5%e5%ad%90/">寒门难再出贵子</A> 
<A title="1 个话题" class="tag-link-57" style="font-size: 8pt;" href="http://www.xperseverance.net/blogs/tag/%e6%97%b6%e6%94%bf%e7%bb%8f%e6%b5%8e/">时政经济</A> 
<A title="3 个话题" class="tag-link-23" style="font-size: 13.384pt;" href="http://www.xperseverance.net/blogs/tag/%e6%98%a5%e7%a7%8b/">春秋</A> 
<A title="1 个话题" class="tag-link-53" style="font-size: 8pt;" href="http://www.xperseverance.net/blogs/tag/%e6%9c%89%e7%94%a8%e7%9a%84%e8%af%9d/">有用的话</A> 
<A title="1 个话题" class="tag-link-34" style="font-size: 8pt;" href="http://www.xperseverance.net/blogs/tag/%e9%9a%8f%e6%83%b3/">随想</A></DIV></ASIDE><ASIDE 
class="widget widget_archive" id="archives-2">
<H3 class="widget-title">文章归档</H3>
<UL>
  <LI><A title="2015 年七月" href="http://www.xperseverance.net/blogs/date/2015/07/">2015 
  年七月</A></LI>
  <LI><A title="2015 年四月" href="http://www.xperseverance.net/blogs/date/2015/04/">2015 
  年四月</A></LI>
  <LI><A title="2014 年十月" href="http://www.xperseverance.net/blogs/date/2014/10/">2014 
  年十月</A></LI>
  <LI><A title="2014 年八月" href="http://www.xperseverance.net/blogs/date/2014/08/">2014 
  年八月</A></LI>
  <LI><A title="2014 年六月" href="http://www.xperseverance.net/blogs/date/2014/06/">2014 
  年六月</A></LI>
  <LI><A title="2014 年五月" href="http://www.xperseverance.net/blogs/date/2014/05/">2014 
  年五月</A></LI>
  <LI><A title="2014 年四月" href="http://www.xperseverance.net/blogs/date/2014/04/">2014 
  年四月</A></LI>
  <LI><A title="2014 年三月" href="http://www.xperseverance.net/blogs/date/2014/03/">2014 
  年三月</A></LI>
  <LI><A title="2014 年二月" href="http://www.xperseverance.net/blogs/date/2014/02/">2014 
  年二月</A></LI>
  <LI><A title="2013 年十一月" href="http://www.xperseverance.net/blogs/date/2013/11/">2013 
  年十一月</A></LI>
  <LI><A title="2013 年十月" href="http://www.xperseverance.net/blogs/date/2013/10/">2013 
  年十月</A></LI>
  <LI><A title="2013 年九月" href="http://www.xperseverance.net/blogs/date/2013/09/">2013 
  年九月</A></LI>
  <LI><A title="2013 年八月" href="http://www.xperseverance.net/blogs/date/2013/08/">2013 
  年八月</A></LI>
  <LI><A title="2013 年七月" href="http://www.xperseverance.net/blogs/date/2013/07/">2013 
  年七月</A></LI>
  <LI><A title="2013 年六月" href="http://www.xperseverance.net/blogs/date/2013/06/">2013 
  年六月</A></LI>
  <LI><A title="2013 年五月" href="http://www.xperseverance.net/blogs/date/2013/05/">2013 
  年五月</A></LI>
  <LI><A title="2013 年四月" href="http://www.xperseverance.net/blogs/date/2013/04/">2013 
  年四月</A></LI>
  <LI><A title="2013 年三月" href="http://www.xperseverance.net/blogs/date/2013/03/">2013 
  年三月</A></LI>
  <LI><A title="2013 年二月" href="http://www.xperseverance.net/blogs/date/2013/02/">2013 
  年二月</A></LI>
  <LI><A title="2012 年十二月" href="http://www.xperseverance.net/blogs/date/2012/12/">2012 
  年十二月</A></LI>
  <LI><A title="2012 年十一月" href="http://www.xperseverance.net/blogs/date/2012/11/">2012 
  年十一月</A></LI>
  <LI><A title="2012 年十月" href="http://www.xperseverance.net/blogs/date/2012/10/">2012 
  年十月</A></LI>
  <LI><A title="2012 年九月" href="http://www.xperseverance.net/blogs/date/2012/09/">2012 
  年九月</A></LI>
  <LI><A title="2012 年七月" href="http://www.xperseverance.net/blogs/date/2012/07/">2012 
  年七月</A></LI>
  <LI><A title="2012 年六月" href="http://www.xperseverance.net/blogs/date/2012/06/">2012 
  年六月</A></LI>
  <LI><A title="2012 年四月" href="http://www.xperseverance.net/blogs/date/2012/04/">2012 
  年四月</A></LI>
  <LI><A title="2012 年三月" href="http://www.xperseverance.net/blogs/date/2012/03/">2012 
  年三月</A></LI>
  <LI><A title="2012 年二月" href="http://www.xperseverance.net/blogs/date/2012/02/">2012 
  年二月</A></LI>
  <LI><A title="2011 年十一月" href="http://www.xperseverance.net/blogs/date/2011/11/">2011 
  年十一月</A></LI>
  <LI><A title="2011 年十月" href="http://www.xperseverance.net/blogs/date/2011/10/">2011 
  年十月</A></LI>
  <LI><A title="2011 年九月" href="http://www.xperseverance.net/blogs/date/2011/09/">2011 
  年九月</A></LI>
  <LI><A title="2011 年五月" href="http://www.xperseverance.net/blogs/date/2011/05/">2011 
  年五月</A></LI>
  <LI><A title="2011 年四月" href="http://www.xperseverance.net/blogs/date/2011/04/">2011 
  年四月</A></LI></UL></ASIDE><ASIDE class="widget widget_categories" id="categories-2">
<H3 class="widget-title">分类目录</H3>
<UL>
  <LI class="cat-item cat-item-3"><A title="查看 Academics 下的所有文章" href="http://www.xperseverance.net/blogs/category/academics/">Academics</A> 
  </LI>
  <LI class="cat-item cat-item-8"><A title="查看 Algorithm 下的所有文章" href="http://www.xperseverance.net/blogs/category/algorithm/">Algorithm</A> 
  </LI>
  <LI class="cat-item cat-item-4"><A title="查看 Chinese Histroy 下的所有文章" href="http://www.xperseverance.net/blogs/category/chinese-histroy/">Chinese 
  Histroy</A> </LI>
  <LI class="cat-item cat-item-48"><A title="查看 Economics 下的所有文章" href="http://www.xperseverance.net/blogs/category/economics/">Economics</A> 
  </LI>
  <LI class="cat-item cat-item-39"><A title="查看 Life 下的所有文章" href="http://www.xperseverance.net/blogs/category/life/">Life</A> 
  </LI>
  <LI class="cat-item cat-item-6"><A title="查看 Techniques 下的所有文章" href="http://www.xperseverance.net/blogs/category/techniques/">Techniques</A> 
  </LI>
  <LI class="cat-item cat-item-7"><A title="查看 WordPress 下的所有文章" href="http://www.xperseverance.net/blogs/category/wordpress/">WordPress</A> 
  </LI>
  <LI class="cat-item cat-item-57"><A title="查看 时政经济 下的所有文章" href="http://www.xperseverance.net/blogs/category/%e6%97%b6%e6%94%bf%e7%bb%8f%e6%b5%8e/">时政经济</A> 
  </LI>
  <LI class="cat-item cat-item-1"><A title="查看 未分类 下的所有文章" href="http://www.xperseverance.net/blogs/category/uncategorized/">未分类</A> 
  </LI>
  <LI class="cat-item cat-item-59"><A title="查看 管理学 下的所有文章" href="http://www.xperseverance.net/blogs/category/%e7%ae%a1%e7%90%86%e5%ad%a6/">管理学</A> 
  </LI></UL></ASIDE><ASIDE class="widget widget_recent_comments" id="recent-comments-2">
<H3 class="widget-title">近期评论</H3>
<UL id="recentcomments">
  <LI class="recentcomments">李铭 发表在《<A href="http://www.xperseverance.net/blogs/2013/03/1744/#comment-1163">Reading 
  Note : Parameter estimation for text analysis 暨LDA学习小结</A>》</LI>
  <LI class="recentcomments"><A class="url" onclick="javascript:_gaq.push(['_trackEvent','outbound-commentauthor','http://vote.weibo.com']);" 
  href="http://vote.weibo.com/ajaxlogin?framelogin=1&amp;callback=parent.sinaSSOController.feedBackUrlCallBack" 
  rel="external nofollow">工程小学生</A> 发表在《<A href="http://www.xperseverance.net/blogs/2013/03/1744/#comment-1162">Reading 
  Note : Parameter estimation for text analysis 暨LDA学习小结</A>》</LI>
  <LI class="recentcomments">wtc1992 发表在《<A href="http://www.xperseverance.net/blogs/2012/03/510/#comment-1161">The 
  Dirichlet Distribution 狄利克雷分布 (PRML 2.2.1)</A>》</LI>
  <LI class="recentcomments">buring 发表在《<A href="http://www.xperseverance.net/blogs/2012/03/21/#comment-1156">PRML 
  Chapter 2. Probability Distributions</A>》</LI>
  <LI class="recentcomments">恒 发表在《<A href="http://www.xperseverance.net/blogs/2013/03/1744/#comment-1155">Reading 
  Note : Parameter estimation for text analysis 
暨LDA学习小结</A>》</LI></UL></ASIDE><ASIDE class="widget widget_links" id="linkcat-26">
<H3 class="widget-title">个人博客</H3>
<UL class="xoxo blogroll">
  <LI><A title="同行赞一个" onclick="javascript:_gaq.push(['_trackEvent','outbound-blogroll','http://sweetglue.info']);" 
  href="http://sweetglue.info/" target="_blank">Hstream</A> 同行赞一个</LI>
  <LI><A title="MIT!大牛！" onclick="javascript:_gaq.push(['_trackEvent','outbound-blogroll','http://freemind.pluskid.org']);" 
  href="http://freemind.pluskid.org/" target="_blank">pluskid</A> MIT!大牛！</LI>
  <LI><A title="传说后宫三千万" onclick="javascript:_gaq.push(['_trackEvent','outbound-blogroll','http://heliang.me']);" 
  href="http://heliang.me/blog/" target="_blank">Roba</A> 传说后宫三千万</LI>
  <LI><A title="世界冠军级神神牛！" onclick="javascript:_gaq.push(['_trackEvent','outbound-blogroll','http://blog.watashi.ws']);" 
  href="http://blog.watashi.ws/" target="_blank">watashi</A> 世界冠军级神神牛！</LI>
  <LI><A title="来而不往非礼也" onclick="javascript:_gaq.push(['_trackEvent','outbound-blogroll','http://blog.csdn.net']);" 
  href="http://blog.csdn.net/yangliuy/article/list/1?viewmode=list" 
  target="">yangliuy</A> 来而不往非礼也</LI>
  <LI><A title="牛人+名人" onclick="javascript:_gaq.push(['_trackEvent','outbound-blogroll','http://www.zhizhihu.com']);" 
  href="http://www.zhizhihu.com/" target="">丕子</A> 牛人+名人</LI></UL></ASIDE><ASIDE 
class="widget widget_links" id="linkcat-25">
<H3 class="widget-title">好友</H3>
<UL class="xoxo blogroll">
  <LI><A title="数学研究所大牛哦" onclick="javascript:_gaq.push(['_trackEvent','outbound-blogroll','http://diaorui.net']);" 
  href="http://diaorui.net/" target="_blank">diaorui</A> 数学研究所大牛哦</LI>
  <LI><A title="想去百度找他" onclick="javascript:_gaq.push(['_trackEvent','outbound-blogroll','http://jimliu.net']);" 
  href="http://jimliu.net/" target="_blank">jimliu</A> 想去百度找他</LI>
  <LI><A title="来神，神人" onclick="javascript:_gaq.push(['_trackEvent','outbound-blogroll','http://licstar.net']);" 
  href="http://licstar.net/" target="_blank">licstar</A> 来神，神人</LI>
  <LI><A title="nexflix 大赛中国第一牛人" onclick="javascript:_gaq.push(['_trackEvent','outbound-blogroll','http://xlvector.net']);" 
  href="http://xlvector.net/" target="_blank">xlvector</A> nexflix 
大赛中国第一牛人</LI></UL></ASIDE><ASIDE class="widget widget_links" id="linkcat-27">
<H3 class="widget-title">学术网站</H3>
<UL class="xoxo blogroll">
  <LI><A title="高信息量网站" onclick="javascript:_gaq.push(['_trackEvent','outbound-blogroll','http://www.lookpre.com']);" 
  href="http://www.lookpre.com/index" target="_blank">lookpre</A> 高信息量网站</LI>
  <LI><A onclick="javascript:_gaq.push(['_trackEvent','outbound-blogroll','http://www.fnlp.org']);" 
  href="http://www.fnlp.org/" target="_blank">复旦大学NLP</A></LI>
  <LI><A onclick="javascript:_gaq.push(['_trackEvent','outbound-blogroll','http://www.52nlp.cn']);" 
  href="http://www.52nlp.cn/" target="_blank">我爱自然语言处理</A></LI></UL></ASIDE><ASIDE 
class="widget widget_links" id="linkcat-32">
<H3 class="widget-title">算法</H3>
<UL class="xoxo blogroll">
  <LI><A onclick="javascript:_gaq.push(['_trackEvent','outbound-blogroll','http://acmicpc.info']);" 
  href="http://acmicpc.info/" 
target="_blank">acmicpc.info</A></LI></UL></ASIDE><ASIDE class="widget widget_meta" 
id="meta-2">
<H3 class="widget-title">功能</H3>
<UL>
  <LI><A 
  href="http://www.xperseverance.net/blogs/wp-login.php?action=register">注册</A></LI>
  <LI><A href="http://www.xperseverance.net/blogs/wp-login.php">登录</A></LI>
  <LI><A title="使用 RSS 2.0 订阅本站点内容" href="http://www.xperseverance.net/blogs/feed/">文章 
  <ABBR title="Really Simple Syndication">RSS</ABBR></A></LI>
  <LI><A title="使用 RSS 订阅本站点的所有文章的近期评论" href="http://www.xperseverance.net/blogs/comments/feed/">评论 
  <ABBR title="Really Simple Syndication">RSS</ABBR></A></LI>
  <LI><A title="基于 WordPress，一个优美、先进的个人信息发布平台。" 
  href="http://cn.wordpress.org/">WordPress.org</A></LI></UL></ASIDE><ASIDE class="widget widget_text" 
id="text-3">
<H3 class="widget-title">访问地图</H3>
<DIV class="textwidget">
<DIV id="clustrmaps-widget"></DIV>
<SCRIPT type="text/javascript">var _clustrmaps = {'url' : 'http://www.xperseverance.net/', 'user' : 1094631, 'server' : '2', 'id' : 'clustrmaps-widget', 'version' : 1, 'date' : '2013-05-13', 'lang' : 'zh', 'corners' : 'square' };(function (){ var s = document.createElement('script'); s.type = 'text/javascript'; s.async = true; s.src = 'http://www2.clustrmaps.com/counter/map.js'; var x = document.getElementsByTagName('script')[0]; x.parentNode.insertBefore(s, x);})();</SCRIPT>
<NOSCRIPT>&lt;a href="http://www2.clustrmaps.com/user/cb410b3e7" 
onclick="javascript:_gaq.push(['_trackEvent','outbound-widget','http://www2.clustrmaps.com']);"&gt;&lt;img 
src="http://www2.clustrmaps.com/stats/maps-no_clusters/www.xperseverance.net--thumb.jpg" 
alt="Locations of visitors to this page" /&gt;&lt;/a&gt;</NOSCRIPT> 
</DIV></ASIDE></DIV><!-- #secondary .widget-area -->			 </DIV><!-- #main -->	 
</DIV><!-- #page --> <FOOTER class="clearfix" id="footer" 
role="contentinfo"><SECTION class="clearfix" id="colophon">
<DIV id="top-scroll"><A title="Scroll to Top" class="scroll" href="http://www.xperseverance.net/blogs/tag/lda/#admired-top">
<DIV id="scroll-top"></DIV></A>		 </DIV>
<DIV id="footer-html"></DIV><!-- #footer-html -->			 
<DIV id="footer-info">
<DIV id="site-info">© 2015					<A title="持之以恒" href="http://www.xperseverance.net/blogs/" 
rel="home">持之以恒					</A>				 </DIV><!-- #site-info -->								 
<DIV id="site-generator"><A href="http://wp-ultra.com/" rel="generator">Admired 
Theme</A>				 </DIV></DIV></SECTION></FOOTER><!-- #footer --> 
<SCRIPT class="javascript" src="LDA%20%20持之以恒_files/shCore.js"></SCRIPT>
 
<SCRIPT class="javascript" src="LDA%20%20持之以恒_files/shBrushCSharp.js"></SCRIPT>
 
<SCRIPT class="javascript" src="LDA%20%20持之以恒_files/shBrushPhp.js"></SCRIPT>
 
<SCRIPT class="javascript" src="LDA%20%20持之以恒_files/shBrushJScript.js"></SCRIPT>
 
<SCRIPT class="javascript" src="LDA%20%20持之以恒_files/shBrushJava.js"></SCRIPT>
 
<SCRIPT class="javascript" src="LDA%20%20持之以恒_files/shBrushVb.js"></SCRIPT>
 
<SCRIPT class="javascript" src="LDA%20%20持之以恒_files/shBrushSql.js"></SCRIPT>
 
<SCRIPT class="javascript" src="LDA%20%20持之以恒_files/shBrushXml.js"></SCRIPT>
 
<SCRIPT class="javascript" src="LDA%20%20持之以恒_files/shBrushDelphi.js"></SCRIPT>
 
<SCRIPT class="javascript" src="LDA%20%20持之以恒_files/shBrushPython.js"></SCRIPT>
 
<SCRIPT class="javascript" src="LDA%20%20持之以恒_files/shBrushRuby.js"></SCRIPT>
 
<SCRIPT class="javascript" src="LDA%20%20持之以恒_files/shBrushCss.js"></SCRIPT>
 
<SCRIPT class="javascript" src="LDA%20%20持之以恒_files/shBrushCpp.js"></SCRIPT>
 
<SCRIPT class="javascript">
dp.SyntaxHighlighter.ClipboardSwf = 'http://www.xperseverance.net/blogs/wp-content/plugins/google-syntax-highlighter/Scripts/clipboard.swf';
dp.SyntaxHighlighter.HighlightAll('code');
</SCRIPT>
 <!--[if IE]>
<script language=javascript>
//<![CDATA[
	if (screen_res=="") {
		screen_res = screen.width + " x " + screen.height;
	}
	if (screen_res!=" x ") {
		var cdate = new Date();
		cdate.setTime(cdate.getTime()+(48*60*60*1000));
		var cexpires = cdate.toGMTString();
		//var the_cookie = "wassup_screen_res="+escape(screen_res)+"; expires="+cexpires;
		document.cookie = "wassup_screen_res=" + escape(screen_res)+ "; path=/; domain=" + document.domain;

	}
//]]>
</script>
<![endif]--><!--
<p class="small"> WassUp 1.8.3.1 timestamp: 2015-08-02 12:51:05PM UTC (08:51PM)<br />
If above timestamp is not current time, this page is cached.</p> --> 
<SCRIPT type="text/javascript">
	jQuery('a[href^="#admired-top"]').live('click',function(event){
		event.preventDefault();
		var target_offset = jQuery(this.hash).offset() ? jQuery(this.hash).offset().top : 0;
		jQuery('html, body').animate({scrollTop:target_offset}, 800);
	});
</SCRIPT>
 </BODY></HTML>
